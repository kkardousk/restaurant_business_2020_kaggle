independence100_duped %>%
group_by(state_cleaned) %>%
summarise(Sales = sum(normalized_sales)) %>%
right_join(data.frame(state_abb = state.abb, # not a fan of right join but this is so all the us map gets drawn/delimited (as we have many states for which we have no data in this small dataset)
state_name = state.name), by = c("state_cleaned" = "state_abb")) %>%
right_join(long_lat, by = c("state_name" = "region"))
# reordering factor levels and sorting order (for accurate drawing if state bounderies)
independence100_map_ready$state_cleaned <- factor(independence100_map_ready$state_cleaned)
independence100_map_ready <- independence100_map_ready[order(independence100_map_ready$order), ] # sorting by order (for drawing states)
p <- ggplot(independence100_map_ready, aes(long, lat, group = group, fill = Sales)) +
geom_polygon(color = "black")
# adding state abbs
centroids <- data.frame(region = tolower(state.name), long = state.center$x, lat = state.center$y)
centroids$abb <- state.abb[match(centroids$region,tolower(state.name))]
# joining map ready df with centroids
independence100_map_ready <-
independence100_map_ready %>%
left_join(centroids %>%
mutate(state_name = str_to_title(region)),
by = c("state_name" = "state_name"))
# adding on state lable layer
map_with_state_labels <-
p +
with(centroids,
annotate(geom = "text", x = long, y = lat, label = abb,
size = 4,color="white",family="Times")
)
map_with_state_labels
map_with_state_labels +
scale_colour_continuous(labels=comma)
map_with_state_labels +
scale_fill_continuous(labels = comma)
map_with_state_labels +
scale_fill_continuous(labels = comma) +
scale_fill_continuous(labels = dollar)
rm(list = ls(all.names = TRUE))
library(tidyverse)
library(janitor)
library(tidytext)
library(textclean)
library(ggplot2)
library(stringi)
library(stringr)
library(scales)
options(scipen = 999)
# reading files after extracting all files (originally zipped)
# setwd("C://Documents and Settings/kardo/Documents/kaggle_files_data/restaurant_business_2020/data/")
list.files()
## exploring datasets
# future50
future_50 <- read.csv("Future50.csv", stringsAsFactors = FALSE)
# Independence100
independence100 <- read.csv("Independence100.csv", stringsAsFactors = FALSE)
# Top250
top250 <- read.csv("Top250.csv", stringsAsFactors = FALSE)
# using independce rating study first to try to visualize map based upon states - to  see any relationship between geography and business growth (positive or negative)
fct_count(independence100$State)
# cleaning up state acronyms (also a chance to consolidate them - florida is spelled differently because of whitespace)
independence100_cleaned <-
independence100 %>%
mutate(state_cleaned = gsub( "\\.", "", str_squish(str_to_lower(State)) ),
state_cleaned = case_when(state_cleaned == "calif" ~ "CA",
state_cleaned == "colo"  ~ "CO",
state_cleaned == "dc"    ~ "DC",
state_cleaned == "fla"   ~ "FL",
state_cleaned == "ga"    ~ "GA",
state_cleaned == "ill"   ~ "IL",
state_cleaned == "ind"   ~ "IN",
state_cleaned == "mass"  ~ "MA",
state_cleaned == "mich"  ~ "MI",
state_cleaned == "nc"    ~ "NC",
state_cleaned == "nj"    ~ "NJ",
state_cleaned == "ny"    ~ "NY",
state_cleaned == "nev"   ~ "NV",
state_cleaned == "ore"   ~ "OR",
state_cleaned == "pa"    ~ "PA",
state_cleaned == "tenn"  ~ "TN",
state_cleaned == "texas" ~ "TX",
state_cleaned == "va"    ~ "VA")
)
# using maps package nad ggplot2 data to extract long and lat for drawing map
library(maps)
long_lat <- map_data("state") # from ggplot - user has to have "maps" installed
long_lat$region <- str_to_title(long_lat$region) # to match string format in independence100 df
# adding state abbs to join on state abbs from independence100_cleaned
independence100_duped <- # note that this will create a duped record because of cartesian join so we will just use the mean to plot numeric data on the map so to not inflate numbers
independence100_cleaned %>%
group_by(Rank, Restaurant) %>%
right_join(data.frame(state_abb = state.abb, # not a fan of right join but this is so all the us map gets drawn/delimited (as we have many states for which we have no data in this small dataset)
state_name = state.name), by = c("state_cleaned" = "state_abb")) %>%
right_join(long_lat, by = c("state_name" = "region")) %>%
mutate(rn = row_number()) %>%
mutate(rn_max = max(rn)) %>%
mutate(normalized_sales = Sales / rn_max) %>%
select(-c(rn, rn_max)) %>%
ungroup() # we will keep this for futher use; but we need to reaggreate and generate aggregation of sales per state
# aggregating at a state level
independence100_map_ready <-
independence100_duped %>%
group_by(state_cleaned) %>%
summarise(Sales = sum(normalized_sales)) %>%
right_join(data.frame(state_abb = state.abb, # not a fan of right join but this is so all the us map gets drawn/delimited (as we have many states for which we have no data in this small dataset)
state_name = state.name), by = c("state_cleaned" = "state_abb")) %>%
right_join(long_lat, by = c("state_name" = "region"))
# reordering factor levels and sorting order (for accurate drawing if state bounderies)
independence100_map_ready$state_cleaned <- factor(independence100_map_ready$state_cleaned)
independence100_map_ready <- independence100_map_ready[order(independence100_map_ready$order), ] # sorting by order (for drawing states)
p <- ggplot(independence100_map_ready, aes(long, lat, group = group, fill = Sales)) +
geom_polygon(color = "black")
# adding state abbs
centroids <- data.frame(region = tolower(state.name), long = state.center$x, lat = state.center$y)
centroids$abb <- state.abb[match(centroids$region,tolower(state.name))]
# joining map ready df with centroids
independence100_map_ready <-
independence100_map_ready %>%
left_join(centroids %>%
mutate(state_name = str_to_title(region)),
by = c("state_name" = "state_name"))
# adding on state lable layer
map_with_state_labels <-
p +
with(centroids,
annotate(geom = "text", x = long, y = lat, label = abb,
size = 4, color = "white", family = "Times")
)
map_with_state_labels +
scale_fill_continuous(labels = comma) + # adding commas to sales figures
scale_fill_continuous(labels = dollar) + # and adding dollars to know it's revenue
ggtitle("Indepedence Top 100 Restaurant Sales in 2020")
map_with_state_labels +
scale_fill_continuous(labels = comma) + # adding commas to sales figures
scale_fill_continuous(labels = dollar) + # and adding dollars to know it's revenue
ggtitle("Indepedence Top 100 Restaurant Sales in 2020") +
theme(plot.title = element_text(hjust = 0.5))
rm(list = ls(all.names = TRUE))
library(tidyverse)
library(janitor)
library(tidytext)
library(textclean)
library(ggplot2)
library(stringi)
library(stringr)
library(scales)
options(scipen = 999)
# reading files after extracting all files (originally zipped)
# setwd("C://Documents and Settings/kardo/Documents/kaggle_files_data/restaurant_business_2020/data/")
list.files()
## exploring datasets
# future50
future_50 <- read.csv("Future50.csv", stringsAsFactors = FALSE)
# Independence100
independence100 <- read.csv("Independence100.csv", stringsAsFactors = FALSE)
# Top250
top250 <- read.csv("Top250.csv", stringsAsFactors = FALSE)
# using independce rating study first to try to visualize map based upon states - to  see any relationship between geography and business growth (positive or negative)
fct_count(independence100$State)
# cleaning up state acronyms (also a chance to consolidate them - florida is spelled differently because of whitespace)
independence100_cleaned <-
independence100 %>%
mutate(state_cleaned = gsub( "\\.", "", str_squish(str_to_lower(State)) ),
state_cleaned = case_when(state_cleaned == "calif" ~ "CA",
state_cleaned == "colo"  ~ "CO",
state_cleaned == "dc"    ~ "DC",
state_cleaned == "fla"   ~ "FL",
state_cleaned == "ga"    ~ "GA",
state_cleaned == "ill"   ~ "IL",
state_cleaned == "ind"   ~ "IN",
state_cleaned == "mass"  ~ "MA",
state_cleaned == "mich"  ~ "MI",
state_cleaned == "nc"    ~ "NC",
state_cleaned == "nj"    ~ "NJ",
state_cleaned == "ny"    ~ "NY",
state_cleaned == "nev"   ~ "NV",
state_cleaned == "ore"   ~ "OR",
state_cleaned == "pa"    ~ "PA",
state_cleaned == "tenn"  ~ "TN",
state_cleaned == "texas" ~ "TX",
state_cleaned == "va"    ~ "VA")
)
# using maps package nad ggplot2 data to extract long and lat for drawing map
library(maps)
long_lat <- map_data("state") # from ggplot - user has to have "maps" installed
long_lat$region <- str_to_title(long_lat$region) # to match string format in independence100 df
# adding state abbs to join on state abbs from independence100_cleaned
independence100_duped <- # note that this will create a duped record because of cartesian join so we will just use the mean to plot numeric data on the map so to not inflate numbers
independence100_cleaned %>%
group_by(Rank, Restaurant) %>%
right_join(data.frame(state_abb = state.abb, # not a fan of right join but this is so all the us map gets drawn/delimited (as we have many states for which we have no data in this small dataset)
state_name = state.name), by = c("state_cleaned" = "state_abb")) %>%
right_join(long_lat, by = c("state_name" = "region")) %>%
mutate(rn = row_number()) %>%
mutate(rn_max = max(rn)) %>%
mutate(normalized_sales = Sales / rn_max) %>%
select(-c(rn, rn_max)) %>%
ungroup() # we will keep this for futher use; but we need to reaggreate and generate aggregation of sales per state
# aggregating at a state level
independence100_map_ready <-
independence100_duped %>%
group_by(state_cleaned) %>%
summarise(Sales = sum(normalized_sales)) %>%
right_join(data.frame(state_abb = state.abb, # not a fan of right join but this is so all the us map gets drawn/delimited (as we have many states for which we have no data in this small dataset)
state_name = state.name), by = c("state_cleaned" = "state_abb")) %>%
right_join(long_lat, by = c("state_name" = "region"))
# reordering factor levels and sorting order (for accurate drawing if state bounderies)
independence100_map_ready$state_cleaned <- factor(independence100_map_ready$state_cleaned)
independence100_map_ready <- independence100_map_ready[order(independence100_map_ready$order), ] # sorting by order (for drawing states)
p <- ggplot(independence100_map_ready, aes(long, lat, group = group, fill = Sales)) +
geom_polygon(color = "black")
# adding state abbs
centroids <- data.frame(region = tolower(state.name), long = state.center$x, lat = state.center$y)
centroids$abb <- state.abb[match(centroids$region,tolower(state.name))]
# joining map ready df with centroids
independence100_map_ready <-
independence100_map_ready %>%
left_join(centroids %>%
mutate(state_name = str_to_title(region)),
by = c("state_name" = "state_name"))
# adding on state lable layer
map_with_state_labels <-
p +
with(centroids,
annotate(geom = "text", x = long, y = lat, label = abb,
size = 4, color = "white", family = "Times")
)
map_with_state_labels +
scale_fill_continuous(labels = comma) + # adding commas to sales figures
scale_fill_continuous(labels = dollar) + # and adding dollars to know it's revenue
ggtitle("Independence Result of Top 100 Restaurant Sales in 2020") +
theme(plot.title = element_text(hjust = 0.5))
rm(list = ls(all.names = TRUE))
library(tidyverse)
library(janitor)
library(tidytext)
library(textclean)
library(ggplot2)
library(stringi)
library(stringr)
library(scales)
options(scipen = 999)
# reading files after extracting all files (originally zipped)
# setwd("C://Documents and Settings/kardo/Documents/kaggle_files_data/restaurant_business_2020/data/")
list.files()
## exploring datasets
# future50
future_50 <- read.csv("Future50.csv", stringsAsFactors = FALSE)
# Independence100
independence100 <- read.csv("Independence100.csv", stringsAsFactors = FALSE)
# Top250
top250 <- read.csv("Top250.csv", stringsAsFactors = FALSE)
# using independce rating study first to try to visualize map based upon states - to  see any relationship between geography and business growth (positive or negative)
fct_count(independence100$State)
# cleaning up state acronyms (also a chance to consolidate them - florida is spelled differently because of whitespace)
independence100_cleaned <-
independence100 %>%
mutate(state_cleaned = gsub( "\\.", "", str_squish(str_to_lower(State)) ),
state_cleaned = case_when(state_cleaned == "calif" ~ "CA",
state_cleaned == "colo"  ~ "CO",
state_cleaned == "dc"    ~ "DC",
state_cleaned == "fla"   ~ "FL",
state_cleaned == "ga"    ~ "GA",
state_cleaned == "ill"   ~ "IL",
state_cleaned == "ind"   ~ "IN",
state_cleaned == "mass"  ~ "MA",
state_cleaned == "mich"  ~ "MI",
state_cleaned == "nc"    ~ "NC",
state_cleaned == "nj"    ~ "NJ",
state_cleaned == "ny"    ~ "NY",
state_cleaned == "nev"   ~ "NV",
state_cleaned == "ore"   ~ "OR",
state_cleaned == "pa"    ~ "PA",
state_cleaned == "tenn"  ~ "TN",
state_cleaned == "texas" ~ "TX",
state_cleaned == "va"    ~ "VA")
)
# using maps package nad ggplot2 data to extract long and lat for drawing map
library(maps)
long_lat <- map_data("state") # from ggplot - user has to have "maps" installed
long_lat$region <- str_to_title(long_lat$region) # to match string format in independence100 df
# adding state abbs to join on state abbs from independence100_cleaned
independence100_duped <- # note that this will create a duped record because of cartesian join so we will just use the mean to plot numeric data on the map so to not inflate numbers
independence100_cleaned %>%
group_by(Rank, Restaurant) %>%
right_join(data.frame(state_abb = state.abb, # not a fan of right join but this is so all the us map gets drawn/delimited (as we have many states for which we have no data in this small dataset)
state_name = state.name), by = c("state_cleaned" = "state_abb")) %>%
right_join(long_lat, by = c("state_name" = "region")) %>%
mutate(rn = row_number()) %>%
mutate(rn_max = max(rn)) %>%
mutate(normalized_sales = Sales / rn_max) %>%
select(-c(rn, rn_max)) %>%
ungroup() # we will keep this for futher use; but we need to reaggreate and generate aggregation of sales per state
# aggregating at a state level
independence100_map_ready <-
independence100_duped %>%
group_by(state_cleaned) %>%
summarise(Sales = sum(normalized_sales)) %>%
right_join(data.frame(state_abb = state.abb, # not a fan of right join but this is so all the us map gets drawn/delimited (as we have many states for which we have no data in this small dataset)
state_name = state.name), by = c("state_cleaned" = "state_abb")) %>%
right_join(long_lat, by = c("state_name" = "region"))
# reordering factor levels and sorting order (for accurate drawing if state bounderies)
independence100_map_ready$state_cleaned <- factor(independence100_map_ready$state_cleaned)
independence100_map_ready <- independence100_map_ready[order(independence100_map_ready$order), ] # sorting by order (for drawing states)
p <- ggplot(independence100_map_ready, aes(long, lat, group = group, fill = Sales)) +
geom_polygon(color = "black")
# adding state abbs
centroids <- data.frame(region = tolower(state.name), long = state.center$x, lat = state.center$y)
centroids$abb <- state.abb[match(centroids$region,tolower(state.name))]
# joining map ready df with centroids
independence100_map_ready <-
independence100_map_ready %>%
left_join(centroids %>%
mutate(state_name = str_to_title(region)),
by = c("state_name" = "state_name"))
# adding on state lable layer
map_with_state_labels <-
p +
with(centroids,
annotate(geom = "text", x = long, y = lat, label = abb,
size = 4, color = "white", family = "Times")
)
map_with_state_labels +
scale_fill_continuous(labels = comma) + # adding commas to sales figures
scale_fill_continuous(labels = dollar) + # and adding dollars to know it's revenue
ggtitle("Independence Result of Top 100 Restaurant in 2020 (by Sales figure)") +
theme(plot.title = element_text(hjust = 0.5))
View(top250)
View(future_50)
sapply(top250, class)
# we first need to convert tot_sales and yoy_units to numeric
top250_cleaned <-
top250 %>%
mutate(yoy_sales = as.numeric(str_replace(YOY_Sales, "%", "")),
YOY_Units = as.numeric(str_replace(YOY_Units, "%", "")))
top250_cleaned <-
top250 %>%
mutate(yoy_sales = as.numeric(str_replace(YOY_Sales, "%", "")),
yoy_units = as.numeric(str_replace(YOY_Units, "%", "")))
View(top250_cleaned)
#keeping only numeric features now to be readu for clustering
top250_cluster_ready <-
top250_cleaned %>%
keep(is.numeric())
#keeping only numeric features now to be readu for clustering
top250_cluster_ready <-
top250_cleaned %>%
keep(is.numeric)
View(top250_cluster_ready)
#keeping only numeric features now to be readu for clustering
top250_cluster_ready <-
top250_cleaned %>%
keep(is.numeric) %>%
select(-1) # rank is not needed - and might throw off algoithm even when we scale
clust_20 <- kmeans(top250_cluster_ready, centers = 2, nstart = 20) # let's start with 2 clusters (small and big business as the assumption + 20 iterations)
clust_20
wssplot <- function(data, nc=15, seed=123){
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i)$withinss)}
plot(1:nc, wss, type="b", xlab="Number of groups",
ylab="Sum of squares within a group")}
wssplot(input, nc = 20)
wssplot(clust_20, nc = 20)
wssplot <- function(data, nc=15, seed=123){
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i)$withinss)}
plot(1:nc, wss, type="b", xlab="Number of groups",
ylab="Sum of squares within a group")}
wssplot(clust_20, nc = 20)
clust_20
wssplot(top250_cluster_ready, nc = 20)
library(cluster)
library(factoextra)
sil <- silhouette(clust_20$cluster, dist(top250_cluster_ready))
fviz_silhouette(sil)
install.packages("factoextra")
library(cluster)
library(factoextra)
sil <- silhouette(clust_20$cluster, dist(top250_cluster_ready))
fviz_silhouette(sil)
install.packages("factoextra")
install.packages("factoextra")
install.packages("factoextra")
library(cluster)
library(factoextra)
sil <- silhouette(clust_20$cluster, dist(top250_cluster_ready))
fviz_silhouette(sil)
rm(list = ls(all.names = TRUE))
library(tidyverse)
library(janitor)
library(tidytext)
library(textclean)
library(ggplot2)
library(stringi)
library(stringr)
library(scales)
options(scipen = 999)
# reading files after extracting all files (originally zipped)
# setwd("C://Documents and Settings/kardo/Documents/kaggle_files_data/restaurant_business_2020/data/")
list.files()
## exploring datasets
# future50
future_50 <- read.csv("Future50.csv", stringsAsFactors = FALSE)
# Independence100
independence100 <- read.csv("Independence100.csv", stringsAsFactors = FALSE)
# Top250
top250 <- read.csv("Top250.csv", stringsAsFactors = FALSE)
# using independce rating study first to try to visualize map based upon states - to  see any relationship between geography and business growth (positive or negative)
fct_count(independence100$State)
# cleaning up state acronyms (also a chance to consolidate them - florida is spelled differently because of whitespace)
independence100_cleaned <-
independence100 %>%
mutate(state_cleaned = gsub( "\\.", "", str_squish(str_to_lower(State)) ),
state_cleaned = case_when(state_cleaned == "calif" ~ "CA",
state_cleaned == "colo"  ~ "CO",
state_cleaned == "dc"    ~ "DC",
state_cleaned == "fla"   ~ "FL",
state_cleaned == "ga"    ~ "GA",
state_cleaned == "ill"   ~ "IL",
state_cleaned == "ind"   ~ "IN",
state_cleaned == "mass"  ~ "MA",
state_cleaned == "mich"  ~ "MI",
state_cleaned == "nc"    ~ "NC",
state_cleaned == "nj"    ~ "NJ",
state_cleaned == "ny"    ~ "NY",
state_cleaned == "nev"   ~ "NV",
state_cleaned == "ore"   ~ "OR",
state_cleaned == "pa"    ~ "PA",
state_cleaned == "tenn"  ~ "TN",
state_cleaned == "texas" ~ "TX",
state_cleaned == "va"    ~ "VA")
)
# using maps package nad ggplot2 data to extract long and lat for drawing map
library(maps)
long_lat <- map_data("state") # from ggplot - user has to have "maps" installed
long_lat$region <- str_to_title(long_lat$region) # to match string format in independence100 df
# adding state abbs to join on state abbs from independence100_cleaned
independence100_duped <- # note that this will create a duped record because of cartesian join so we will just use the mean to plot numeric data on the map so to not inflate numbers
independence100_cleaned %>%
group_by(Rank, Restaurant) %>%
right_join(data.frame(state_abb = state.abb, # not a fan of right join but this is so all the us map gets drawn/delimited (as we have many states for which we have no data in this small dataset)
state_name = state.name), by = c("state_cleaned" = "state_abb")) %>%
right_join(long_lat, by = c("state_name" = "region")) %>%
mutate(rn = row_number()) %>%
mutate(rn_max = max(rn)) %>%
mutate(normalized_sales = Sales / rn_max) %>%
select(-c(rn, rn_max)) %>%
ungroup() # we will keep this for futher use; but we need to reaggreate and generate aggregation of sales per state
# aggregating at a state level
independence100_map_ready <-
independence100_duped %>%
group_by(state_cleaned) %>%
summarise(Sales = sum(normalized_sales)) %>%
right_join(data.frame(state_abb = state.abb, # not a fan of right join but this is so all the us map gets drawn/delimited (as we have many states for which we have no data in this small dataset)
state_name = state.name), by = c("state_cleaned" = "state_abb")) %>%
right_join(long_lat, by = c("state_name" = "region"))
# reordering factor levels and sorting order (for accurate drawing if state bounderies)
independence100_map_ready$state_cleaned <- factor(independence100_map_ready$state_cleaned)
independence100_map_ready <- independence100_map_ready[order(independence100_map_ready$order), ] # sorting by order (for drawing states)
p <- ggplot(independence100_map_ready, aes(long, lat, group = group, fill = Sales)) +
geom_polygon(color = "black")
# adding state abbs
centroids <- data.frame(region = tolower(state.name), long = state.center$x, lat = state.center$y)
centroids$abb <- state.abb[match(centroids$region,tolower(state.name))]
# joining map ready df with centroids
independence100_map_ready <-
independence100_map_ready %>%
left_join(centroids %>%
mutate(state_name = str_to_title(region)),
by = c("state_name" = "state_name"))
# adding on state lable layer
map_with_state_labels <-
p +
with(centroids,
annotate(geom = "text", x = long, y = lat, label = abb,
size = 4, color = "white", family = "Times")
)
map_with_state_labels +
scale_fill_continuous(labels = comma) + # adding commas to sales figures
scale_fill_continuous(labels = dollar) + # and adding dollars to know it's revenue
ggtitle("Independence Result of Top 100 Restaurant in 2020 (by Sales figures)") +
theme(plot.title = element_text(hjust = 0.5))
## now we explore what makes a small versus big business - clustering comes to mind
# clustering to see if unsupervised method gives us a clear distinction between small and big entitites
sapply(top250, class)
# we first need to convert tot_sales and yoy_units to numeric
top250_cleaned <-
top250 %>%
mutate(yoy_sales = as.numeric(str_replace(YOY_Sales, "%", "")),
yoy_units = as.numeric(str_replace(YOY_Units, "%", "")))
#keeping only numeric features now to be readu for clustering
top250_cluster_ready <-
top250_cleaned %>%
keep(is.numeric) %>%
select(-1) # rank is not needed - and might throw off algoithm even when we scale
set.seed(123) # for reproducibility
clust_20 <- kmeans(top250_cluster_ready, centers = 2, nstart = 20) # let's start with 2 clusters (small and big business as the assumption + 20 iterations)
clust_20 # flairly low within cluster score "compacteness" score ~= 60%
# let's use below function to then plot optimal no. of clusters to choose from
wssplot <- function(data, nc=15, seed=123){
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i)$withinss)}
plot(1:nc, wss, type="b", xlab="Number of groups",
ylab="Sum of squares within a group")}
wssplot(top250_cluster_ready, nc = 20) # 2 is acutally not a bad start
library(cluster)
library(factoextra)
sil <- silhouette(clust_20$cluster, dist(top250_cluster_ready))
fviz_silhouette(sil)
